WIKI_PAGE_DEPTHS = {
    'Sherutni_patrin': 0, 'Vikipidiya': 1, 'Lekh': 1, 'Romane_manusha': 1, 'Romani_chib': 1,
    'Standardizuimi_Romani_chib': 1, 'Romano_lekhipen': 1, 'Chiba_le_romenge': 1, 'Patrinipen_le_bare_Romengo': 1,
    'Romano_siklyaripen': 1, 'Kale': 1, 'Sinti': 1, 'Poraimos': 1, 'Pativ': 1, 'Desi': 1, 'Banjara': 1, 'Devnagrī': 1,
    'Chexanipen': 1, 'Phuvipen': 1, 'Sikavdimos': 1, 'Compyutereske_janglimata': 1, 'Chhibavipen': 1,
    'Jivaniakohramovipen': 1, 'Mesto_software': 2, 'Sel': 2, 'Sudutni_Asiya': 2, 'Pakistan': 2, 'Bharat': 2,
    'Mashkarutne_Indo-Ariyane_chhiba': 2, 'Shuto_Orizari': 2, 'Republika_Makedoniya': 2, 'Gav': 2, 'Foro': 2,
    'Budeshti': 2, 'Chhib': 2, 'Chave': 2, 'Rekshan': 2, 'Them': 2, 'Jermaniya': 2, 'Bari_Britaniya': 2,
    'Spaniya': 2, 'Norvejiya': 2, 'Valshenengi_Romani_chhib': 2, 'Standardizuyimi_Romani_qhib_(Selahetin_Kruezi)': 2,
    'Bulgariya': 2, 'Rusiya': 2, 'Rumuniya': 2, 'Chexiya': 2, 'Ungariya': 2, 'Nicolaye_Gyorge': 2,
    'Viktoriya_Mohaci': 2, 'Liviya_Yaroka': 2, 'Cedomir_Yovanovic': 2, 'Shtefan_Răzvan': 2, 'Yanosh_Bogdan': 2,
    'Raiko_Juric': 2, 'Deliya_Grigore': 2, 'Selahetin_Kruezi': 2, 'Ronald_Lee': 2, 'Mateo_Maximov': 2,
    'Hedina_Sijercic': 2, 'Katarina_Taikon': 2, 'Vasile_Yonesko': 2, 'Grigorash_Diniku': 2, 'Django_Reinhardt': 2,
    'Yon_Voiku': 2, 'Romika_Puchanu': 2, 'Esma_Rejepova': 2, 'Azis': 2, 'Sandro': 2, 'Shaban_Bayramovic': 2,
    'Reyhan': 2, 'Sofi_Marinova': 2, 'Vali_Vizheliye': 2, 'Adriyan_Minune': 2, 'Zvonko_Demirovic': 2,
    'Fazliya': 2, 'Haso_Menovin_Frohlish': 2, 'Zhean_Konstantin': 2, 'Shtefan_Bǎnikǎ': 2, 'Xoakin_Kortes': 2,
    'Valery_Novoselsky': 2, 'Johann_Trollmann': 2, 'Rosa_Taikon': 2, 'Sikingro%27Kher': 2, 'Europa': 2,
    'Romano_siklyaripen_la_Rumuniyatar': 2, 'Romano_siklyaripen_le_Slovaikostar': 2,
    'Romano_siklyaripen_la_Ungariyatar': 2, 'Franchiya': 2, 'Portugaliya': 2, 'Indo-Europikane_chhiba': 2,
    'Kontinento': 2, 'Andalusiya': 2, 'Kalo': 2, 'Shelbersh': 2, 'Italiya': 2, 'Transilvaniya': 2, 'Slovaiko': 2,
    'Manush': 2, 'Nasho': 2, 'Phuv': 2, 'Nordutni_Amerika': 2, 'ABCD': 2, 'Seloro': 2, 'Jegeya': 2, 'Chexay': 2,
    'Shiyaron': 2, 'Samodor': 2, 'Phuvipnaske_patrinimata': 2, 'Chhibavipnaski_familiya': 2, 'Afrika': 3,
    'Sahel': 3, 'Sudutni_Amerika': 3, 'Eurasiya': 3, 'Britanikane_dvipa': 3,
    'Khetanipen_la_Sudutne_Asiyako_vash_o_Perutno_Somkerdipen': 3, 'Sherutno_foro': 3,
    'Patrinipen_le_themengo_thai_le_durutne_umalengo_palal_lengo_baripen': 3,
    'Patrinipen_le_themengo_palal_o_gin_le_manushengo': 3,
    'Patrinipen_le_themengo_palal_o_butvaripen_le_manushengo': 3,
    'Patrinipen_le_themengo_palal_o_indekso_le_manushutne_baryaripnasko': 3,
    'Mumbai': 3, 'Patrinipen_le_prinjarde_chhibango_andi_Pharatiya': 3, 'Bersh': 3,
    'Staturya_thai_teritorurya_la_Indiyake': 3, 'Skopiye': 3, 'Ketnepen': 3, 'Europikano_Ekipen': 3,
    'Buxlyarimos_le_Europikane_Ekipnasko': 3, 'Austriya': 3,
    'Phandlo_Thagaripen_la_Bare_Britaniyako_thai_le_Nordutne_Irlandesko': 3, 'Beljiya': 3, 'Danemarka': 3,
    'Estoniya': 3, 'Finland': 3, 'Yelenistan': 3, 'Republika_Irland': 3, 'Latviya': 3, 'Lituaniya': 3,
    'Luksemburgo': 3, 'Malta': 3, 'Olanda': 3, 'Polska': 3, 'Turkiya': 3, 'Patrinipen_le_themengo': 3,
    'Andorra': 3, 'Elveciya': 3, 'Moldova': 3, 'Shkiperiya': 3, 'Ukraina': 3, 'Vatican': 3, 'Dvipa_Faroe': 3,
    'Thanipen': 3, 'Zhudetso': 3, 'Rumunikani_chhib': 3, 'Bukureshti': 3,
    'Patrinipen_le_thanimatengo_la_Rumuniyake_kai_beshen_but_roma': 3, 'Stato': 3, 'Shagede': 3, 'Berlin': 3,
    'Jermanikani_chib': 3, 'Derya': 3, 'Paxro': 3, 'Dvip': 3,
    'Organizaciya_le_Tratatosko_le_Nordutne_Atlantikosko': 3, 'Sofiya': 3, 'Otomano_Thagaripen': 3,
    'Rusikani_chhib': 3, 'Chexiko_stago_(fana)': 3, 'Praga': 3, 'Peshta': 3, 'Ungarikani_chib': 3, 'London': 3,
    '1971': 3, 'Gandhi_(mashkarutni_shkola_ando_Pech)': 3, 'Pech': 3, 'Kanada': 3, 'Aven_Amentza': 3,
    'Rumunikano_lekhipen': 3, 'Euroroma': 3, 'Arxentina': 3, 'Chalga': 3, 'Menix': 3, 'Madrid': 3,
    'Flamenko': 3, 'Israel': 3, 'Roma_Virtual_Network': 3, 'Kher': 3, 'Romanipen': 3,
    'Gandhiskri_shkola_(Zvolenostar)': 3, 'Paris': 3, 'Córdoba,_Spaniya': 3, 'Nad_Tatrou_sa_blýska': 3,
    'Vasho': 3, 'Shero': 3, 'Dimashko': 3, 'Kham-Sestemi': 3, 'Zaro': 3, 'Kham': 3, 'Budor': 3, 'Shukor': 3,
    'Manjor': 3, 'Kuror': 3, 'Shani': 3, 'Rahor': 3, 'Ketor': 3, 'Pharnovon': 3,
    'Patrinipen_le_dvipengo_palal_o_baripen': 3, 'Angluni_chhib': 3, 'Korkori_chhib': 3,
    'Chhibavipnasko_azbalipen': 3, 'Xamomi_chhib': 3, 'Patrinipen_le_chhibavipnaske_familiyengo': 3,
    'Phandle_Staturya_la_Amerikiyake': 4, 'Patrinipen_le_themengo_le_buteder_sherutne_forurenca': 4,
    'Patrinipen_le_sherutne_forurengo': 4, 'Patrinipen_le_themengo_kai_o_sherutno_foro_nai_o_bareder_foro': 4,
    'Washington,_D.C.': 4, 'Brazil': 4, 'Mexiko': 4, 'Peru': 4, 'Venezuela': 4, 'Chile': 4, 'Paraguay': 4,
    'Ekuador': 4, 'Uruguay': 4, 'Sudutni_Koreya': 4, 'Kolombiya': 4, 'Nordutni_Koreya': 4, 'Kuba': 4,
    'Boliviya': 4, 'Palestina': 4, 'Butvaripen_le_manushengo': 4, 'Indekso_le_manushutne_baryaripnasko': 4,
    'Distrikturya_la_Indiyake': 4, 'Patrinipen_le_distrikturengo_la_Indiyake': 4, 'Ekhipnasko_Teritoryo': 4,
    'Nordutno_Irland': 4, 'Irland_(dvip)': 4, 'Brusel': 4, 'Tallinn': 4, 'China': 4, 'Mêsire': 4, 'Kiev': 4,
    'Romanikane_chhiba': 4, 'Latinikano_lekhipen': 4, 'Diskriminaciya': 4, 'New_York_City': 4, 'Daki_chib': 4,
    'Okeyanu': 4, 'Slavikane_chhiba': 4, 'Krimeya': 4, 'Uralikane_chhiba': 4, 'Mashkarthemutno_Romano_Kongreso': 4,
    'Gelem,_Gelem': 4, 'Styago_le_romengo': 4, 'Mahatma_Gandhi': 4, 'Buenos_Aires': 4, 'Indo-Aryano': 4,
    'Zvolen': 4, 'Santiago,_Chile': 5, 'Valparaíso': 5, 'Belfast': 5, 'Lima': 5, 'Lisbon': 5, 'Montevideo': 5,
    'Brasília': 5, 'Ho_Chi_Minh_City': 5, 'Thagaripen_Inka': 5, 'Bogotá': 5, 'Tehsil': 5, 'Baro_Belfast': 5,
    'Asia': 5, 'Prinjardi_chhib': 5, 'Patrinipen_le_internetoske_umalurengo_(TLD)': 5, 'Kali_Deryav': 5
}
WIKI_PAGE_DEPTHS = {}

print(len(WIKI_PAGE_DEPTHS))

import requests
from bs4 import BeautifulSoup

base = 'https://rmy.wikipedia.org'
dict = {}
pages = set()
def walk(adress, deepth):
    global pages
    global dict
    try:
        req = requests.get(adress)
        if req.status_code == 200:
            soup = BeautifulSoup(req.text, "lxml")

            adress_name = soup.findAll('a', attrs={"dir": "ltr"})[0].text
            if adress_name.find('rmy.wikipedia.org') > -1:
                begin = adress_name.find('title=')
                if begin > -1:
                    if adress_name.find('&') != -1:
                        name = adress_name[begin + 6:adress_name.find('&')]
                    else:
                        name = adress_name[begin + 6:]
                    if name.find(':') == -1:
                        deep = dict.setdefault(name, deepth)
                        if deepth < deep:
                            dict[name] = deepth
            print(deepth)
            tags = soup.body.find_all('a')
            #print('here')
            for tag in tags:
                #print(tag)
                if tag.get('href'):
                    if (tag.get('href').find('/wiki/') > -1) and\
                            (tag.get('href').find('https://') == -1) and \
                            (tag.get('href').find('.png') == -1) and \
                            (tag.get('href').find('.jpg') == -1) and \
                            (tag.get('href').find(':') == -1) and \
                            (tag.get('href').find('#') == -1) and \
                            (tag.get('href').find('?') == -1) and \
                            (tag.get('href').find('=') == -1) and \
                            (tag.get('href').find('&') == -1) and \
                            (tag.get('href').find('.gif') == -1) and \
                            (tag.get('href').find('.svg') == -1) and \
                            (tag.get('href').find('//') == -1):
                        if tag.get('href') not in pages:
                            pages.add(tag.get('href'))
                            print(base + tag.get('href'))
                            yield base + tag.get('href')
                            #walk(base + tag.get('href'), deepth + 1)
            #print('end')
        else:
            return False
    except BaseException:
        print('fail')
    return False




def bfs(adress):
    level = [adress]
    new_level = []
    deepth = 0
    while len(level) > 0:
        for page in level:
            res = list(walk(page, deepth))
            if res:
                new_level.extend(res)
        deepth += 1
        level = new_level.copy()
        new_level = []
        print('---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------')
        print(dict)

#walk('https://rmy.wikipedia.org/wiki/Kale', 0)

bfs('https://rmy.wikipedia.org/wiki/Sherutni_patrin')
print(dict)


# rsp = requests.get('https://rmy.wikipedia.org/wiki/Sherutni_patrin')
# print(rsp.status_code)
# rsp = requests.get('https://rmy.wikipedia.org/wiki/Kale')
# print(rsp.status_code)
# soup = BeautifulSoup(rsp.text, "lxml")
# print(soup.body.prettify())
# tags = soup.body.find_all('a')
# for tag in tags:
#    print(tag.get('href'))
# req = requests.get(base + '/wiki/Franchiya')
# print(req.status_code)

"""
rsp = requests.get('https://rmy.wikipedia.org/wiki/Sherutni_patrin')
print(rsp.status_code)

soup = BeautifulSoup(rsp.text, "lxml")
#print(soup.prettify())
tags = soup.find_all('a')
#for tag in tags:
#    print(tag.get('href'))

adress_name = soup.findAll('a', attrs={"dir": "ltr"})[0].text
if adress_name.find('rmy.wikipedia.org') > -1:
    begin = adress_name.find('title=')
    name = adress_name[begin + 6:adress.find('&')]
    print(name)
print(soup.findAll('a', attrs={"dir": "ltr"})[0].text)

"""

Это есть вики-краулер (программу, обходящую заданный сегмент интернета по ссылкам),
 который должен обойти статьи википедии (по умолчанию румынской),
  достижимые со [стартовой страницы](по умолчанию http://rmy.wikipedia.org).
Для каждой найденной статьи посчитанны её удалённость от главной страницы.
 Удалённость стартовой страницы равна 0. Если на страницу можно попасть разными способами, будем брать
  кратчайшее расстояние.
* Служебные ссылки и ссылки на несуществующие страницы не входят в ответ.
* Для скачивания странички пользуемся библиотекой `requests`, для парсинга html – `BeautifulSoup`.
* Всего статей примерно 300(в румынской), работать кроулер должен совсем не долго.
* запускать для любой другой вики без ограничения на максимальный уровень глубины равный <= 2 не рекомендуется
* в случае отказа доступа к сайту, краулер пытается использовать Proxy (осторожно, не стоит запускать с максимальным уровенем глубины равный > 1)
чтобы запустить краулер:
```json
>python crawler.py
```

чтобы запустить краулер с дополнительными параметрами:
```json
>python crawler.py adress lang max_deepth
```
где adress определяет стартовую страницу, lang определяет язык вики по которой мы ищем и max_deepth определяет максимальную глубину поиска


```json
{
    "<title 1>": <deepth>,
    "<title 2>": <deepth 2>
}
```

где title – атрибут страницы в BeautifulSoup извлекаемый из
```python
soup.findAll('a', attrs={"dir": "ltr"})[0].text
```

Пример:
```json
{
    "Sherutni_patrin": 0,
    "Romano_lekhipen": 1,
    "Sinti": 1
}
```


